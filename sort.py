# -*- coding: utf-8 -*-
"""sort.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A1y-I3i42Ko6XPGzVehlEJ6rOo9BInB4

Mount to google drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""link images here"""

base_dir = '/content/drive/MyDrive/Fashion'
train_dir = base_dir  # Since your folders are directly in the project folder

# Directory with our training recyclable pictures
train_recyclable_dir = os.path.join(train_dir, 'recyclable')

# Directory with our training non-recyclable pictures
train_non_recyclable_dir = os.path.join(train_dir, 'non_recyclable')

# For validation, you can split your data or use the same folders
validation_dir = base_dir
validation_recyclable_dir = train_recyclable_dir
validation_non_recyclable_dir = train_non_recyclable_dir

"""Load to see if images show up"""

from PIL import Image
import matplotlib.pyplot as plt

# Load a sample image
img_path = os.path.join(recyclable_path, os.listdir(recyclable_path)[0])
img = Image.open(img_path)
plt.imshow(img)
plt.title("Sample Recyclable Image")
plt.show()

"""Code"""

train_recyclable_fnames = os.listdir(train_recyclable_dir)
print("Recyclable images:")
print(train_recyclable_fnames[:10])

train_non_recyclable_fnames = os.listdir(train_non_recyclable_dir)
train_non_recyclable_fnames.sort()
print("\nNon-recyclable images:")
print(train_non_recyclable_fnames[:10])

"""Let's find out the total number of recyclable and non-recyclable images:"""

print('total training recyclable images:', len(os.listdir(train_recyclable_dir)))
print('total training non-recyclable images:', len(os.listdir(train_non_recyclable_dir)))
print('total validation recyclable images:', len(os.listdir(validation_recyclable_dir)))
print('total validation non-recyclable images:', len(os.listdir(validation_non_recyclable_dir)))

"""Now let's take a look at a few pictures to get a better sense of what the datasets look like:"""

# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Parameters for our graph; we'll output images in a 4x4 configuration
nrows = 4
ncols = 4

# Index for iterating over images
pic_index = 0

"""Display a batch of recyclable and non-recyclable pictures:"""

# Set up matplotlib fig, and size it to fit 4x4 pics
fig = plt.gcf()
fig.set_size_inches(ncols * 4, nrows * 4)

pic_index += 8
next_recyclable_pix = [os.path.join(train_recyclable_dir, fname)
                       for fname in train_recyclable_fnames[pic_index-8:pic_index]]
next_non_recyclable_pix = [os.path.join(train_non_recyclable_dir, fname)
                           for fname in train_non_recyclable_fnames[pic_index-8:pic_index]]

for i, img_path in enumerate(next_recyclable_pix+next_non_recyclable_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

"""## Building the CNN Model

The images that will go into our convnet are 150x150 color images.

We will stack 3 {convolution + relu + maxpooling} modules. Our convolutions operate on 3x3 windows
and our maxpooling layers operate on 2x2 windows. Our first convolution extracts 16 filters,
the following one extracts 32 filters, and the last one extracts 64 filters.
"""

from tensorflow.keras import layers
from tensorflow.keras import Model

"""The CNN model follows a typical architecture for image classification tasks.
It starts with convolutional and pooling layers to extract and reduce features,
followed by dense layers to process and classify the features.
"""

# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for
# the three color channels: R, G, and B
img_input = layers.Input(shape=(150, 150, 3))

# First convolution extracts 16 filters that are 3x3
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(16, 3, activation='relu')(img_input)
x = layers.MaxPooling2D(2)(x)

# Second convolution extracts 32 filters that are 3x3
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(32, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

# Third convolution extracts 64 filters that are 3x3
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(64, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

"""We end our network with a sigmoid activation, so that the output will be a single scalar
between 0 and 1, encoding the probability that the image is non-recyclable (vs recyclable).
"""

# Flatten feature map to a 1-dim tensor so we can add fully connected layers
x = layers.Flatten()(x)

# Create a fully connected layer with ReLU activation and 512 hidden units
x = layers.Dense(512, activation='relu')(x)

# Create output layer with a single node and sigmoid activation
output = layers.Dense(1, activation='sigmoid')(x)

# Create model:
# input = input feature map
# output = input feature map + stacked convolution/maxpooling layers + fully
# connected layer + sigmoid output layer
model = Model(img_input, output)

"""Compile the model with binary_crossentropy loss and RMSprop optimizer:"""

from tensorflow.keras.optimizers import RMSprop

model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(learning_rate=0.001),
              metrics=['acc'])

"""## Data Preprocessing and Augmentation

Since we have limited training data (only 10 images per class), we'll use data augmentation
to artificially increase our dataset size and prevent overfitting. Data augmentation applies
random transformations to the images during training.
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,           # Normalize pixel values to [0, 1]
    rotation_range=40,        # Randomly rotate images by up to 40 degrees
    width_shift_range=0.2,    # Randomly shift images horizontally
    height_shift_range=0.2,   # Randomly shift images vertically
    shear_range=0.2,          # Apply shearing transformations
    zoom_range=0.2,           # Randomly zoom in on images
    horizontal_flip=True,     # Randomly flip images horizontally
    fill_mode='nearest')      # Fill in new pixels after transformations

# Validation data generator (no augmentation, only rescaling)
val_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images in batches
train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=10,  # Smaller batch size due to limited data
        class_mode='binary')  # Binary labels for recyclable (0) vs non-recyclable (1)

# Flow validation images in batches
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=10,
        class_mode='binary')

"""## Training

Let's train the model. With only 20 images total, we'll use more epochs to allow
the model to learn despite the limited data. Note: With such a small dataset,
results may vary significantly between runs.
"""

history = model.fit(
      train_generator,
      steps_per_epoch=2,  # 20 images / batch_size of 10 = 2 steps
      epochs=50,  # More epochs to compensate for small dataset
      validation_data=validation_generator,
      validation_steps=2,
      verbose=2)

"""## Visualize Training Results

Let's plot the training and validation accuracy/loss over epochs:
"""

import matplotlib.pyplot as plt

# Retrieve accuracy and loss from training history
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc))

plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.show()

"""## Predict on New Images

To predict whether a new image is recyclable or non-recyclable:
"""

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

# Load the image - UPDATE THIS PATH to your test image
image_path = '/content/drive/MyDrive/Fashion/test_image.jpg'
img = load_img(image_path, target_size=(150, 150))

# Convert the image to a numpy array
x = img_to_array(img)

# Reshape the numpy array to match the expected input shape of the model
x = x.reshape((1,) + x.shape)

# Rescale by 1/255
x /= 255

# Display the image
plt.imshow(img)
plt.axis('off')
plt.show()

# Predict the class label of the image
prediction = model.predict(x)
print(f"\nPrediction value: {prediction[0][0]:.4f}")

# Interpret the prediction
# The model outputs a value between 0 and 1
# Values closer to 0 indicate recyclable, values closer to 1 indicate non-recyclable
if prediction[0][0] > 0.5:
    print("Predicted class: Non-Recyclable")
    print(f"Confidence: {prediction[0][0]*100:.2f}%")
else:
    print("Predicted class: Recyclable")
    print(f"Confidence: {(1-prediction[0][0])*100:.2f}%")

"""web scrap images for recyclable and non recyclable --> get more data

"""